# Rook Cluster Values

# Enable cluster deployment
enabled: true

# Namespace for cluster
namespace: rook-ceph

# Cluster configuration
cluster:
  enabled: true
  spec:
    # Ceph version
    cephVersion:
      image: quay.io/ceph/ceph:v18.2.0
    
    # Data directory host path
    dataDirHostPath: /var/lib/rook
    
    # Monitor configuration
    mon:
      count: 3
      allowMultiplePerNode: false
    
    # Dashboard configuration
    dashboard:
      enabled: true
      ssl: true
    
    # Network configuration
    network:
      hostNetwork: false
      provider: host
    
    # Health check configuration
    healthCheck:
      daemonHealth:
        mon:
          interval: 45s
          timeout: 25s
        osd:
          interval: 60s
          timeout: 25s
        status:
          interval: 60s
          timeout: 25s
    
    # Prometheus monitoring
    monitoring:
      enabled: true
      rulesNamespace: rook-ceph
    
    # Crash collector
    crashCollector:
      disable: false
    
    # Placement configuration for Ceph daemons
    placement:
      # All daemons (mon, mgr, osd) - overrides individual settings
      all:
        # Node selector for all daemons
        nodeSelector:
          ceph-storage: "true"
        # Tolerations for all daemons
        tolerations:
        - key: "node-role.caas.com/platform-worker"
          operator: "Exists"
          effect: "NoExecute"
        # Affinity for all daemons
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: ceph-storage
                  operator: In
                  values:
                  - "true"
                - key: node-role.caas.com/platform-worker
                  operator: Exists
        # Pod anti-affinity for all daemons
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - rook-ceph-mon
                  - rook-ceph-mgr
                  - rook-ceph-osd
              topologyKey: kubernetes.io/hostname
    
    # Storage configuration - customizable section
    storage:
      # Use all nodes in the cluster
      useAllNodes: true
      # Use all devices on each node
      useAllDevices: false
      # Device filter (regex pattern)
      # deviceFilter: "^sd."
      
      # Global configuration for all OSDs
      config:
        # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
        # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
        # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
        # osdsPerDevice: "1" # this value can be overridden at the node or device level
        # encryptedDevice: "false" # the default value for this option is "false"
        # deviceClass: "myclass" # specify a device class for OSDs in the cluster
      
      # Whether to allow changing the device class of an OSD after it is created
      allowDeviceClassUpdate: false
      # Whether to allow resizing the OSD crush weight after osd pvc is increased
      allowOsdCrushWeightUpdate: false
      
      # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false.
      # Then, only the named nodes below will be used as storage resources.
      # Each node's 'name' field should match their 'kubernetes.io/hostname' label.
      nodes:
        # Example node configurations - customize these for your environment
        # - name: "node1.example.com"
        #   devices: # specific devices to use for storage can be specified for each node
        #     - name: "vdb"
        #     - name: "nvme01" # multiple osds can be created on high performance devices
        #       config:
        #         osdsPerDevice: "5"
        #     - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
        #   config: # configuration can be specified at the node level which overrides the cluster level config
        # - name: "node2.example.com"
        #   deviceFilter: "^sd."

# Filesystem configuration (for CephFS RWX storage)
filesystem:
  enabled: true
  name: myfs
  # Number of active MDS instances
  activeCount: 1
  # Number of standby MDS instances
  standbyCount: 1
  # Metadata pool configuration
  metadataPool:
    replicated:
      size: 3
  # Data pool configuration
  dataPools:
    - name: replicated
      replicated:
        size: 3

# Storage Classes configuration
storageClasses:
  # CephFS Storage Class (ReadWriteMany)
  cephfs:
    enabled: true
    name: rook-cephfs
    # Default storage class for RWX
    isDefault: true
    # Reclaim policy
    reclaimPolicy: Delete
    # Volume binding mode
    volumeBindingMode: Immediate
    # Allow volume expansion
    allowVolumeExpansion: true
    # Pool name
    pool: myfs-replicated

# Toolbox configuration
toolbox:
  enabled: true
  image: quay.io/ceph/ceph:v18.2.0

# Test PVC configuration
testPvc:
  enabled: true
  size: 1Gi
  image: busybox:1.35

# Global settings
global:
  # Image registry
  imageRegistry: ""
  # Image pull policy
  imagePullPolicy: IfNotPresent
  # Image pull secrets
  imagePullSecrets: []
  
  # Node selector - Simple way to target specific nodes
  # Example:
  # nodeSelector:
  #   ceph-storage: "true"
  nodeSelector: {}
  
  # Tolerations - Allow pods to be scheduled on nodes with specific taints
  # Example:
  # tolerations:
  # - key: "node-role.caas.com/platform-worker"
  #   operator: "Exists"
  #   effect: "NoExecute"
  tolerations: []
  
  # Affinity - More complex scheduling rules
  # Example:
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: ceph-storage
  #           operator: In
  #           values:
  #         - "true"
  affinity: {}
  
  # Additional annotations (optional)
  annotations: {}
  
  # Additional labels (optional)
  labels: {} 